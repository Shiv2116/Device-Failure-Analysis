# -*- coding: utf-8 -*-
"""Maintenance Cost Reduction through Predictive Techniques

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UqjfT-aHBCTMXyEF9eqFJNfKKuJXaLJN

# Maintenance cost reduction through predictive techniques

BACKGROUND A company has a fleet of devices transmitting daily
sensor readings. They would like to create a predictive maintenance
solution to proactively identify when maintenance should be
performed. This approach promises cost savings over routine or time based preventive maintenance, because tasks are performed only when warranted. 

GOAL: You are tasked with building a predictive model using machine
learning to predict the probability of a device failure. When building
this model, be sure to minimize false positives and false negatives. The
column you are trying to Predict is called failure with binary value 0 for
non-failure and 1 for failure.

# Data Exploration

Importing Modules
"""

import pandas_profiling

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

pd.DataFrame()

"""# Load data as dataframe and data overview"""

df = pd.read_csv('predictive_maintenance_dataset.csv')

df.head()

pandas_profiling.ProfileReport(df)

"""Identiyfing Number of Missing Values"""

df.isnull().sum()

"""Checking Imbalance in the Dataset"""

df.failure.value_counts()

"""Checking the duplicate columns in the data set"""

df.duplicated().sum()

"""Identifying the unique number of total devices and failure devices in the dataset"""

total_devices = len(df.device.unique())
print('There are {} total devices'.format(total_devices))

total_failure_devices = len(df[df.failure == 1].device.unique())
print('There are {} total failure devices'.format(total_failure_devices))

df.nunique()

"""# Summary
This dataset is clean, no missing values. All attributes are integer data type.

It is imbalanced data set, as the failuer class is about 0.1% of unfailure class.Here oversampling approach is used to deal with imbalanced dataset.

metric 7 and 8 seems like exactly same to each other, we can drop one of them.

Some attributes have limited number of distictive values, very sparse, indicating that they are likely to be categorical variable, such as metric3, 5,7,9.


metric 2,3,4,7,9 are highly skewed.

metric differ in their magnitudes. Scaling or centering is requried

# Data Engineering

'Date' Exploration

Creating features based on date

Splitting the date column in Month and Weekdays

Also, Calculating the active days of all the devices considering 2015-01-01 as the minimum or starting date
"""

df.date = pd.to_datetime(df.date)

df['activedays']=df.date-df.date[0]

df['month']=df['date'].dt.month
df['week_day']=df.date.dt.weekday
df['week_day'].replace(0,7,inplace=True)
df

"""Grouping the unique device numbers in Months"""

df.groupby('month').agg({'device':lambda x: x.nunique()})

"""# Data visualization in date"""

df.groupby('month').agg({'device':lambda x: x.nunique()}).plot()
plt.show()

"""This figure shows that as time move on, the number of devices are getting less and less."""

ax = sns.countplot(x="month", hue="failure", data=df)
plt.show()

"""This figure shows most of the devices failed in the first month."""

ax = sns.countplot(x='week_day',hue='failure',data=df)
plt.show()

"""This figure shows that there is no device fails on Saturday. Maybe they don't work on the this day."""

df.groupby('activedays')['device'].count().plot()
plt.show()

"""One can see that the number of devices decreases as time goes by. And there is a big jump in the middle of activedays. Those may be some devices got put back in after they failed and fixed well. We will investigate the detail later."""

max(df.date), min(df.date)

"""All of these data are collected between 11/02/2015 and 01/01/2015

# Devices come back to use
"""

df_date = df.groupby('device').agg({'date':max})

df_date.date.to_dict()

df_failure = df.loc[df.failure==1,['device','date']]

df_good = df.loc[df.failure==0,['device','date']]

df_date.shape,df_failure.shape

df['max_date']=df.device.map(df_date.date.to_dict())

df

dff=df[(df.failure==1)&(df.date!=df.max_date)]

dff

"""Max date means the last day the device got checked. If the max day is ahead of failure date, it means this device returned to use after failed because got fixed."""

fig = plt.figure(figsize=(15,5))
fig.add_subplot(3, 2, 1) 
plt.plot(df.loc[df['device']=='S1F136J0',['failure','month']]['month'],df.loc[df['device']=='S1F136J0',\
         ['failure','month']]['failure'],\
         color = 'red')
fig.add_subplot(3, 2, 2) 
plt.plot(df.loc[df['device']=='W1F0KCP2',['failure','month']]['month'],df.loc[df['device']=='W1F0KCP2',\
         ['failure','month']]['failure'],\
         color = 'red')
fig.add_subplot(3, 2, 3)
plt.plot(df.loc[df['device']=='W1F0M35B',['failure','month']]['month'],df.loc[df['device']=='W1F0M35B',\
         ['failure','month']]['failure'],\
         color = 'red')
fig.add_subplot(3, 2, 4)
plt.plot(df.loc[df['device']=='S1F0GPFZ',['failure','month']]['month'],df.loc[df['device']=='S1F0GPFZ',\
         ['failure','month']]['failure'],\
         color = 'red')
fig.add_subplot(3, 2, 5)
plt.plot(df.loc[df['device']=='W1F11ZG9',['failure','month']]['month'],df.loc[df['device']=='W1F11ZG9',\
         ['failure','month']]['failure'],\
         color = 'red')

plt.show()

"""# Reduce dataset with Unique Device ID"""

df.head()

df.metric1.nunique()

df1 = df.groupby('device').agg({'date':max})

df1.shape

df1

df1=df1.reset_index()

df=df.reset_index(drop=True) 

df2= pd.merge(df1,df,how='left',on=['device','date'])

df2.shape

df2

"""# Create feature called 'failure_before'

If we just take the last record for the devices, we may lose information from those come back after failed ones
"""

df2['failure_before']=0

df2.head()

df2.loc[df2.device == 'S1F136J0','failure_before'] = 1
df2.loc[df2.device == 'W1F0KCP2','failure_before'] = 1
df2.loc[df2.device == 'W1F0M35B','failure_before'] = 1
df2.loc[df2.device == 'S1F0GPFZ','failure_before'] = 1
df2.loc[df2.device == 'W1F11ZG9','failure_before'] = 1

"""# Redefine device ID Value"""

df2.device

Id = df2.device.values.tolist()

"""Changing device ID for first four characters"""

Id1 = [] 
for i in Id:
    i = i[:4]
    Id1.append(i)

df2.device=Id1

df2.device.value_counts()

dev=pd.crosstab(df2['device'],df2['failure'])

dev.div(dev.sum(1).astype(float), axis=0).plot(kind="bar")
plt.show()

"""From the Barplot we can say that Devices ID which begins with ZIF1 fails the most, then W1F1 second

# Data Transformation
"""

df2

df2.nunique()

"""As metric 3, 4, 5, 7, 9 are very small we shall convert this as a categorical data"""

cat_ftrs = ['metric3','metric4', 'metric5', 'metric7', 'metric9'] 
for col in cat_ftrs:
    df2[col]=df2[col].astype('object')

df2.info()

"""Now, Converting the activedays datatype to numerical datatype"""

def str_to_num(str):
    return str.split(' ')[0]

df2.activedays = df2.activedays.astype('str')

df2.activedays=df2.activedays.apply(str_to_num)
df2.activedays = df2.activedays.astype('int')
df2.info()

"""Also, Converting month and week_days in categorical data"""

for col in ['month','week_day']:
    df2[col]=df2[col].astype('object')

df2.info()

"""# Data Standardization"""

df2.head()

"""Data Normalization"""

f, axarr = plt.subplots(1,2) 
sns.distplot(df2['metric1'],ax=axarr[0]) 
axarr[0].set_title('Skewed Distribution') 
sns.distplot(np.log(1+df2['metric1']),ax=axarr[1]) 
axarr[1].set_title('Log-Transformed Distribution')

plt.show()

f, axarr = plt.subplots(1,2) 

sns.distplot(df2['metric2'],ax=axarr[0]) 
axarr[0].set_title('Skewed Distribution') 
sns.distplot(np.log(1+df2['metric2']),ax=axarr[1]) 
axarr[1].set_title('Log-Transformed Distribution')

plt.show()

f, axarr = plt.subplots(1,2) 
sns.distplot(df2['metric6'],ax=axarr[0]) 
axarr[0].set_title('Skewed Distribution') 
sns.distplot(np.log(1+df2['metric6']),ax=axarr[1]) 
axarr[1].set_title('Log-Transformed Distribution')

plt.show()

"""Data Standardization"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

num_ftrs =['metric1','metric2','metric6'] 
df2[num_ftrs]=scaler.fit_transform(df2[num_ftrs])

df2.info()

f, axarr = plt.subplots(1,2) 
sns.distplot(df2['metric1'],ax=axarr[0]) 
axarr[0].set_title('Skewed Distribution') 
sns.distplot(np.log(1+df2['metric1']),ax=axarr[1]) 
axarr[1].set_title('Log-Transformed Distribution')

plt.show()

"""Here, we see that metric1, metric2 and metric6 are scaled.

# Drop Unimportant and Redundant Features
"""

sns.pairplot(df)
plt.show()

"""It is obvious that metric7 and metric8 is highly linear related or equal to each other"""

(df['metric7']==df['metric8']).value_counts()

"""Thus, dropping metric8 column from the dataset"""

df.drop('metric8',axis=1,inplace=True)

df.head()

df2.head()

df2.drop(['date','max_date'],axis=1,inplace=True)

df2.head()

df2.info()

"""Now, creating dummies of categorical datatype"""

df2 = pd.get_dummies(df2,drop_first=True)

df2.head()

df2.shape

df2.failure.value_counts()

"""# Feature Selection

# Defining dependent and independent values
"""

X = df2.drop('failure',axis=1)
Y = df2.failure

X

Y

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(n_estimators=50, max_features='auto')
clf= clf.fit(X,Y)

features = pd.DataFrame()
features['feature']= X.columns
features['important']=clf.feature_importances_
features.sort_values(by=['important'], ascending=False,inplace=True)
features.set_index('feature', inplace=True)
features.iloc[:20,:].plot(kind='barh', figsize=(30,30))

plt.show()

from sklearn.feature_selection import SelectFromModel

model = SelectFromModel(clf,prefit=True)
x_reduced = model.transform(X)
print (x_reduced.shape)

x_reduced=pd.DataFrame(x_reduced)

x_reduced.head()

"""# Model Training

# Logistic Regression Model (LoR)
"""

from sklearn.linear_model import LogisticRegression

log = LogisticRegression()

"""Splitting the dataset into train and test"""

from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state = 42)

"""Fitting the LoR model in our data"""

log.fit(X_train, Y_train)

"""Predicting the test dataset values from the trained dataset """

predictions = log.predict(X_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix

accuracy_score(Y_test, predictions)

""" We see that Logistic Regression gives us an efficiency of about 92% """

print(classification_report(Y_test, predictions))
print(confusion_matrix(Y_test, predictions))

"""Applying Hyperparameter on our LoR model"""

from sklearn.linear_model import LogisticRegression
log = LogisticRegression(solver='newton-cg')
log.fit(X_train, Y_train)

predictions = log.predict(X_test)

accuracy_score(Y_test, predictions)

"""* We noticed that after applying hyperparameter 'newton-cg' on our LoR model, the efficiency of the product became 93.16 %"""

print(classification_report(Y_test, predictions))
print(confusion_matrix(Y_test, predictions))

"""# K Nearest Neighbour Model (KNN)"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, Y_train)

y_pred = knn.predict(X_test)

print(metrics.accuracy_score(Y_test, y_pred))

print(classification_report(Y_test, y_pred))
print(confusion_matrix(Y_test, y_pred))

"""After applying KNN we noticed that it is giving us an accuracy of 94.87% on our dataset.

# Observations

* Accuracy on Logistic Regression Model: 92%
* Accuracy on LoR after applying hyperparameter tuning: 93.16%
* Accuracy on K - Nearest Neighbour Model: 95%
"""